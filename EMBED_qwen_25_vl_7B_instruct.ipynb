{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "284815\n",
      "3186\n",
      "195611\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root = \"/root/letractien/data/nas07/Dataset/Image/EMBED/\"\n",
    "jpgs = \"jpgs\"\n",
    "annotated_mass = \"annotated_mass\"\n",
    "other_jpgs = \"other_jpgs\"\n",
    "csv_files = \"csv_files\"\n",
    "\n",
    "root_list = os.listdir(root)\n",
    "jpgs_list = os.listdir(os.path.join(root, jpgs))\n",
    "annotated_mass_list = os.listdir(os.path.join(root, annotated_mass))\n",
    "other_jpgs_list = os.listdir(os.path.join(root, other_jpgs))\n",
    "csv_files_list = os.listdir(os.path.join(root, csv_files))\n",
    "\n",
    "print(len(root_list))\n",
    "print(len(jpgs_list))\n",
    "print(len(annotated_mass_list))\n",
    "print(len(other_jpgs_list))\n",
    "print(len(csv_files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5, 6\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "CACHE_DIR = \"/root/letractien/Mammo-VLM/.cache\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "model_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_path, \n",
    "    # min_pixels=256*28*28, \n",
    "    # max_pixels=1280*28*28,,\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import io\n",
    "import ast\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL import ImageColor\n",
    "import xml.etree.ElementTree as ET\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "def plot_bounding_boxes(img, bounding_boxes, input_width, input_height, save_path=None):\n",
    "\n",
    "    width, height = img.size\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    colors = [\n",
    "        'red',\n",
    "        'green',\n",
    "        'blue',\n",
    "        'yellow',\n",
    "        'orange',\n",
    "        'pink',\n",
    "        'purple',\n",
    "        'brown',\n",
    "        'gray',\n",
    "        'beige',\n",
    "        'turquoise',\n",
    "        'cyan',\n",
    "        'magenta',\n",
    "        'lime',\n",
    "        'navy',\n",
    "        'maroon',\n",
    "        'teal',\n",
    "        'olive',\n",
    "        'coral',\n",
    "        'lavender',\n",
    "        'violet',\n",
    "        'gold',\n",
    "        'silver',\n",
    "    ] + additional_colors\n",
    "\n",
    "    bounding_boxes = parse_json(bounding_boxes)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    try:\n",
    "        json_output = ast.literal_eval(bounding_boxes)\n",
    "    except Exception as e:\n",
    "        end_idx = bounding_boxes.rfind('\"}') + len('\"}')\n",
    "        truncated_text = bounding_boxes[:end_idx] + \"]\"\n",
    "        json_output = ast.literal_eval(truncated_text)\n",
    "\n",
    "    for i, bounding_box in enumerate(json_output):\n",
    "        color = colors[i % len(colors)]\n",
    "        abs_y1 = int(bounding_box[\"bbox_2d\"][1]/input_height * height)\n",
    "        abs_x1 = int(bounding_box[\"bbox_2d\"][0]/input_width * width)\n",
    "        abs_y2 = int(bounding_box[\"bbox_2d\"][3]/input_height * height)\n",
    "        abs_x2 = int(bounding_box[\"bbox_2d\"][2]/input_width * width)\n",
    "\n",
    "        if abs_x1 > abs_x2:\n",
    "            abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "\n",
    "        if abs_y1 > abs_y2:\n",
    "            abs_y1, abs_y2 = abs_y2, abs_y1\n",
    "\n",
    "        draw.rectangle(\n",
    "            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
    "        )\n",
    "\n",
    "        if \"label\" in bounding_box:\n",
    "            draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n",
    "            \n",
    "    img.show()\n",
    "    if save_path is not None:\n",
    "        img.save(save_path)\n",
    "\n",
    "def parse_json(json_output):\n",
    "    lines = json_output.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"```json\":\n",
    "            json_output = \"\\n\".join(lines[i+1:])  \n",
    "            json_output = json_output.split(\"```\")[0]\n",
    "            break\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(img_url, prompt, system_prompt=\"You are a helpful assistant in detecting suspicious breast lumps or microcalcifications in mammography images\", max_new_tokens=1024):\n",
    "    image = Image.open(img_url)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"image\": img_url\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(\"Output: \\n\", output_text[0])\n",
    "\n",
    "    input_height = inputs['image_grid_thw'][0][1]*14\n",
    "    input_width = inputs['image_grid_thw'][0][2]*14\n",
    "\n",
    "    return output_text[0], input_height, input_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import preprocess\n",
    "\n",
    "save_dir = \"out/EMBED_QW7B\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "root = \"/root/letractien/data/nas07/Dataset/Image/EMBED/\"\n",
    "jpgs = \"jpgs\"\n",
    "annotated_mass = \"annotated_mass\"\n",
    "other_jpgs = \"other_jpgs\"\n",
    "\n",
    "log_path = os.path.join(save_dir, \"log.txt\")\n",
    "img_dir = os.path.join(root, jpgs)\n",
    "count = 0\n",
    "\n",
    "for idx, file in enumerate(os.listdir(img_dir)):\n",
    "    img_in_path = os.path.join(img_dir, file)\n",
    "    img = Image.open(img_in_path)\n",
    "    img_arr = np.array(img)\n",
    "\n",
    "    x, m = preprocess.crop(img_arr, annotation=None)\n",
    "    x, m, _ = preprocess.pad_image_to_square(x, mask_array=m, annotation=None)\n",
    "    x, m, _ = preprocess.resize_image(x, mask_array=m, annotation=None, output_shape=(640, 640))\n",
    "    norm = preprocess.truncation_normalization(x, m)\n",
    "\n",
    "    step1 = preprocess.median_denoise(norm, disk_radius=3)\n",
    "    step2 = preprocess.unsharp_enhance(step1, radius=1.0, amount=1.5)\n",
    "    step3 = preprocess.morphological_tophat(step2, selem_radius=15)\n",
    "    step4 = preprocess.non_local_means_denoise(step3, patch_size=5, patch_distance=6, h_factor=0.8)\n",
    "    step5 = preprocess.wavelet_enhancement(step4, wavelet='db8', level=1)\n",
    "    final = preprocess.clahe(step5, clip_limit=0.02)\n",
    "\n",
    "    disp = preprocess.normalize_for_display(final)\n",
    "    disp = np.nan_to_num(disp)\n",
    "\n",
    "    img_out_path = os.path.join(save_dir, f\"{os.path.splitext(file)[0]}.png\")\n",
    "    Image.fromarray(disp).convert(\"RGB\").save(img_out_path)\n",
    "\n",
    "    disp = preprocess.normalize_for_display(norm)\n",
    "    disp = np.nan_to_num(disp)\n",
    "    img_out_path = os.path.join(save_dir, f\"{os.path.splitext(file)[0]}.png\")\n",
    "    Image.fromarray(disp).convert(\"RGB\").save(img_out_path)\n",
    "\n",
    "    with open(\"PROMPT_EMBED.txt\", \"r\", encoding=\"utf-8\") as f: prompt = f.read()\n",
    "    response, input_height, input_width = inference(img_out_path, prompt)\n",
    "    print(input_height, input_width)\n",
    "\n",
    "    image = Image.open(img_out_path)\n",
    "    print(image.size)\n",
    "\n",
    "    image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "    img_out_path = os.path.join(save_dir, f\"{os.path.splitext(file)[0]}_bbox.png\")\n",
    "    plot_bounding_boxes(image, response, input_width, input_height, img_out_path)\n",
    "\n",
    "    if count == 100: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-mammography",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
