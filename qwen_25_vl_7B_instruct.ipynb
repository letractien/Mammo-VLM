{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5, 6, 7\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "CACHE_DIR = \"/root/letractien/Mammo-VLM/.cache\"\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\",\n",
    "    # quantization_config=bnb_config,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_path, \n",
    "    # min_pixels=256*28*28, \n",
    "    # max_pixels=1280*28*28,\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import io\n",
    "import ast\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL import ImageColor\n",
    "import xml.etree.ElementTree as ET\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "def plot_bounding_boxes(img, bounding_boxes, input_width, input_height):\n",
    "\n",
    "    width, height = img.size\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    colors = [\n",
    "        'red',\n",
    "        'green',\n",
    "        'blue',\n",
    "        'yellow',\n",
    "        'orange',\n",
    "        'pink',\n",
    "        'purple',\n",
    "        'brown',\n",
    "        'gray',\n",
    "        'beige',\n",
    "        'turquoise',\n",
    "        'cyan',\n",
    "        'magenta',\n",
    "        'lime',\n",
    "        'navy',\n",
    "        'maroon',\n",
    "        'teal',\n",
    "        'olive',\n",
    "        'coral',\n",
    "        'lavender',\n",
    "        'violet',\n",
    "        'gold',\n",
    "        'silver',\n",
    "    ] + additional_colors\n",
    "\n",
    "    bounding_boxes = parse_json(bounding_boxes)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    try:\n",
    "        json_output = ast.literal_eval(bounding_boxes)\n",
    "    except Exception as e:\n",
    "        end_idx = bounding_boxes.rfind('\"}') + len('\"}')\n",
    "        truncated_text = bounding_boxes[:end_idx] + \"]\"\n",
    "        json_output = ast.literal_eval(truncated_text)\n",
    "\n",
    "    for i, bounding_box in enumerate(json_output):\n",
    "        color = colors[i % len(colors)]\n",
    "        abs_y1 = int(bounding_box[\"bbox_2d\"][1]/input_height * height)\n",
    "        abs_x1 = int(bounding_box[\"bbox_2d\"][0]/input_width * width)\n",
    "        abs_y2 = int(bounding_box[\"bbox_2d\"][3]/input_height * height)\n",
    "        abs_x2 = int(bounding_box[\"bbox_2d\"][2]/input_width * width)\n",
    "\n",
    "        if abs_x1 > abs_x2:\n",
    "            abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "\n",
    "        if abs_y1 > abs_y2:\n",
    "            abs_y1, abs_y2 = abs_y2, abs_y1\n",
    "\n",
    "        draw.rectangle(\n",
    "            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
    "        )\n",
    "\n",
    "        if \"label\" in bounding_box:\n",
    "            draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n",
    "\n",
    "    img.show()\n",
    "\n",
    "def parse_json(json_output):\n",
    "    lines = json_output.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"```json\":\n",
    "            json_output = \"\\n\".join(lines[i+1:])  \n",
    "            json_output = json_output.split(\"```\")[0]\n",
    "            break\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(img_url, prompt, system_prompt=\"You are a helpful assistant\", max_new_tokens=1024):\n",
    "    image = Image.open(img_url)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"image\": img_url\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(\"Output: \\n\", output_text[0])\n",
    "\n",
    "    input_height = inputs['image_grid_thw'][0][1]*14\n",
    "    input_width = inputs['image_grid_thw'][0][2]*14\n",
    "\n",
    "    return output_text[0], input_height, input_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dataset\n",
    "\n",
    "image_annotation_tuples = dataset.load_image_annotation_tuples()\n",
    "unique_tuples = list({img_path: (img_path, ann) for img_path, ann in image_annotation_tuples}.values())\n",
    "\n",
    "save_dir = \"out/qwen_25_vl_7B_instruct_ipynb\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "log_path = os.path.join(save_dir, \"log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "import preprocess\n",
    "\n",
    "idx = 5\n",
    "img_path, annotation = unique_tuples[idx]\n",
    "folder = annotation['study_id']\n",
    "os.makedirs(os.path.join(save_dir, folder), exist_ok=True)\n",
    "\n",
    "basename = annotation['image_id']\n",
    "img_png_path = os.path.join(save_dir, folder, f\"{basename}.png\")\n",
    "\n",
    "ds = pydicom.dcmread(img_path)\n",
    "img_arr = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "x, m, new_annotation = preprocess.crop(img_arr, annotation=annotation)\n",
    "x, m, new_annotation = preprocess.pad_image_to_square(x, mask_array=m, annotation=new_annotation)\n",
    "x, m, new_annotation = preprocess.resize_image(x, mask_array=m, annotation=new_annotation, output_shape=(640, 640))\n",
    "norm = preprocess.truncation_normalization(x, m)\n",
    "\n",
    "step1 = preprocess.median_denoise(norm, disk_radius=3)\n",
    "step2 = preprocess.unsharp_enhance(step1, radius=1.0, amount=1.5)\n",
    "step3 = preprocess.morphological_tophat(step2, selem_radius=15)\n",
    "step4 = preprocess.non_local_means_denoise(step3, patch_size=5, patch_distance=6, h_factor=0.8)\n",
    "step5 = preprocess.wavelet_enhancement(step4, wavelet='db8', level=1)\n",
    "final = preprocess.clahe(step5, clip_limit=0.02)\n",
    "\n",
    "disp = preprocess.normalize_for_display(final)\n",
    "disp = np.nan_to_num(disp)\n",
    "\n",
    "# new_annotation[\"xmin\"] = max(int(new_annotation['xmin']) - 20, 0)\n",
    "# new_annotation[\"ymin\"] = max(int(new_annotation['ymin']) - 20, 0)\n",
    "# new_annotation[\"xmax\"] = min(int(new_annotation['xmax']) + 20, new_annotation[\"width\"])\n",
    "# new_annotation[\"ymax\"] = min(int(new_annotation['ymax']) + 20, new_annotation[\"height\"])\n",
    "# disp = preprocess.draw_bbox_grayscale(disp.copy(), new_annotation, color=255, thickness=5)\n",
    "\n",
    "img_png_path_pre = os.path.join(save_dir, folder, f\"{basename}_{idx}_preprocessed.png\")\n",
    "Image.fromarray(disp).convert(\"RGB\").save(img_png_path_pre)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4, 4)) \n",
    "plt.imshow(disp, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompt\n",
    "\n",
    "image_path = img_png_path_pre\n",
    "prompt = \"Please strictly mark and select all small, round suspicious masses or suspicious calcifications in the image, and output the corresponding detection frame coordinates for subsequent diagnosis and analysis. The detection frame should fit closely to the detected target, output its bbox coordinates using JSON format.\"\n",
    "response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "image = Image.open(image_path)\n",
    "print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "plot_bounding_boxes(image, response, input_width, input_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (img_path, annotation) in enumerate(unique_tuples):\n",
    "    img_path, annotation = unique_tuples[idx]\n",
    "    folder = annotation['study_id']\n",
    "    os.makedirs(os.path.join(save_dir, folder), exist_ok=True)\n",
    "\n",
    "    basename = annotation['image_id']\n",
    "    img_png_path = os.path.join(save_dir, folder, f\"{basename}.png\")\n",
    "\n",
    "    ds = pydicom.dcmread(img_path)\n",
    "    try: img_arr = ds.pixel_array.astype(np.float32)\n",
    "    except Exception as e: continue \n",
    "\n",
    "    x, m, new_annotation = preprocess.crop(img_arr, annotation=annotation)\n",
    "    x, m, new_annotation = preprocess.pad_image_to_square(x, mask_array=m, annotation=new_annotation)\n",
    "    x, m, new_annotation = preprocess.resize_image(x, mask_array=m, annotation=new_annotation, output_shape=(640, 640))\n",
    "    norm = preprocess.truncation_normalization(x, m)\n",
    "\n",
    "    step1 = preprocess.median_denoise(norm, disk_radius=3)\n",
    "    step2 = preprocess.unsharp_enhance(step1, radius=1.0, amount=1.5)\n",
    "    step3 = preprocess.morphological_tophat(step2, selem_radius=15)\n",
    "    step4 = preprocess.non_local_means_denoise(step3, patch_size=5, patch_distance=6, h_factor=0.8)\n",
    "    step5 = preprocess.wavelet_enhancement(step4, wavelet='db8', level=1)\n",
    "    final = preprocess.clahe(step5, clip_limit=0.02)\n",
    "\n",
    "    disp = preprocess.normalize_for_display(final)\n",
    "    disp = np.nan_to_num(disp)\n",
    "\n",
    "    # new_annotation[\"xmin\"] = max(int(new_annotation['xmin']) - 20, 0)\n",
    "    # new_annotation[\"ymin\"] = max(int(new_annotation['ymin']) - 20, 0)\n",
    "    # new_annotation[\"xmax\"] = min(int(new_annotation['xmax']) + 20, new_annotation[\"width\"])\n",
    "    # new_annotation[\"ymax\"] = min(int(new_annotation['ymax']) + 20, new_annotation[\"height\"])\n",
    "    # disp = preprocess.draw_bbox_grayscale(disp.copy(), new_annotation, color=255, thickness=1)\n",
    "\n",
    "    img_png_path_pre = os.path.join(save_dir, folder, f\"{basename}_{idx}_preprocessed.png\")\n",
    "    Image.fromarray(disp).save(img_png_path_pre)\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.figure(figsize=(4, 4)) \n",
    "    # plt.imshow(disp, cmap='gray')\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    image_path = img_png_path_pre\n",
    "    with open(\"PROMPT_qwen_25_vl_7B_instruct.txt\", \"r\", encoding=\"utf-8\") as f: prompt = f.read()\n",
    "    response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    print(image.size)\n",
    "\n",
    "    image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "    plot_bounding_boxes(image, response, input_width, input_height)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-mammography",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
